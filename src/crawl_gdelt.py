# =====================================
# ğŸ“„ crawl_gdelt.py
# Crawl tin tá»©c vá» Google tá»« GDELT
# =====================================

import pandas as pd
import requests
import io
import os
from datetime import datetime, timedelta


def crawl_gdelt(keyword="Google", days=365, output_path="../data/gdelt_news.csv"):
    print(f"ğŸš€ Crawling GDELT for keyword: {keyword} (last {days} days)")
    base_url = "http://api.gdeltproject.org/api/v2/doc/doc"

    # Táº¡o danh sÃ¡ch ngÃ y chia nhá» Ä‘á»ƒ trÃ¡nh timeout
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=days)
    delta = timedelta(days=7)  # má»—i láº§n crawl 7 ngÃ y

    all_data = []

    while start_date < end_date:
        next_date = min(start_date + delta, end_date)
        start_str = start_date.strftime("%Y%m%d%H%M%S")
        end_str = next_date.strftime("%Y%m%d%H%M%S")

        params = {
            "query": f'"{keyword}"',
            "mode": "ArtList",
            "maxrecords": 250,
            "format": "CSV",
            "startdatetime": start_str,
            "enddatetime": end_str,
        }

        try:
            r = requests.get(base_url, params=params, timeout=30)
            if r.status_code == 200 and len(r.text) > 0:
                df = pd.read_csv(io.StringIO(r.text))
                if not df.empty:
                    all_data.append(df)
                    print(f"âœ… {len(df)} articles from {start_str[:8]} â†’ {end_str[:8]}")
            else:
                print(f"âš ï¸ No data from {start_str[:8]} â†’ {end_str[:8]}")
        except Exception as e:
            print(f"âŒ Error for {start_str[:8]}: {e}")

        start_date = next_date

    # Há»£p nháº¥t toÃ n bá»™ dá»¯ liá»‡u
    if all_data:
        result = pd.concat(all_data, ignore_index=True)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        result.to_csv(output_path, index=False)
        print(f"\nğŸ¯ Done! Saved {len(result)} articles â†’ {output_path}")
    else:
        print("\nâŒ No articles found!")


if __name__ == "__main__":
    crawl_gdelt(keyword="Google", days=365)
